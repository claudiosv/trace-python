
# -*- coding: utf-8 -*-
"""Tokenizer-Quicktour

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sVUP6YwJhocDn6SS_2Na2KIKU5iBugDi
"""

#!mkdir data

#!pip install transformers tokenizers datasets

import torch
print(f"CUDA: {torch.cuda.is_available()}")

from tokenizers import Tokenizer
from tokenizers.models import WordLevel
from tokenizers import normalizers
from tokenizers.normalizers import Lowercase, NFD, StripAccents
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.processors import BertProcessing
from tokenizers.trainers import WordLevelTrainer
from tokenizers.pre_tokenizers import Split
from tokenizers.normalizers import Strip
from tokenizers import Regex

exp = Regex("(^((\w)+(?=\s)))|((\[ENTRY\]\ (\w|\||\.)+)\s)|((\[CALL\]\ (\w|\||\.|\s)+)(?=\ \[))+|(\[EXIT\])")

pre_tokenizer = Split(pattern=exp, behavior="removed",invert=True)
#print(pre_tokenizer.pre_tokenize_str("performExpensiveLogSetup [ENTRY] void [CALL] java.io.PrintStream println java.lang.String void [CALL] java.lang.Math pow double|double double [CALL] java.lang.Math sqrt double double [CALL] java.io.PrintStream println java.lang.String void [EXIT]"))

trace_tokenizer = Tokenizer(WordLevel(unk_token="[UNK]"))
trace_tokenizer.add_special_tokens(["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
trace_tokenizer.normalizer = Strip()
trace_tokenizer.pre_tokenizer = pre_tokenizer
trace_tokenizer.post_processor = BertProcessing(sep=("[SEP]", 0),cls=("[CLS]", 1))
VOCAB_SIZE = 10000
#trace_tokenizer.add_tokens([' '])
trainer = WordLevelTrainer(
    vocab_size=VOCAB_SIZE, special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]
)
files = ["10k_smaller_dataset.txt", "tiny_eval.txt"]
trace_tokenizer.train(files, trainer)

trace_tokenizer.save("data/trace_tokenizer.json")

# print(pre_tokenizer.pre_tokenize_str("performExpensiveLogSetup [ENTRY] void [CALL] java.io.PrintStream println java.lang.String void [CALL] java.lang.Math pow double|double double [CALL] java.lang.Math sqrt double double [CALL] java.io.PrintStream println java.lang.String void [EXIT]"))

from transformers import BertConfig, BertForMaskedLM
scale_factor = 0.25
config = BertConfig(
    vocab_size=VOCAB_SIZE,
    max_position_embeddings=int(768*scale_factor),
    intermediate_size=int(2048*scale_factor),
    hidden_size=int(512*scale_factor),
    num_attention_heads=8,
    num_hidden_layers=6,
    type_vocab_size=5,
    hidden_dropout_prob=0.1,
    attention_probs_dropout_prob=0.1,
)

from transformers import PreTrainedTokenizerFast
fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=trace_tokenizer,
                                         return_special_tokens_mask=True, mask_token='[MASK]', return_token_type_ids=False)
fast_tokenizer.add_special_tokens({'pad_token': '[PAD]', 'mask_token': '[MASK]'})

from datasets import load_dataset
dataset = load_dataset('text', data_files={'train': '10k_smaller_dataset.txt', 'test': 'tiny_eval.txt', 'eval': 'tiny_eval.txt'})
small_train_dataset = dataset["train"]
small_eval_dataset = dataset["test"]

# small_eval_dataset

model = BertForMaskedLM(config)
model.tokenizer = fast_tokenizer

def preprocess_function(examples):
    return fast_tokenizer(examples["text"], max_length = 128, truncation=True, padding=True)

encoded_dataset_train = small_train_dataset.map(preprocess_function, batched=True)
encoded_dataset_test = small_eval_dataset.map(preprocess_function, batched=True)

# trace_tokenizer.encode("filterSuite [ENTRY] junit.framework.TestSuite junit.framework.TestSuite [CALL] java.util.Enumeration hasMoreElements  boolean [CALL] java.util.Enumeration nextElement  java.lang.Object [CALL] java.util.Enumeration hasMoreElements  boolean [CALL] java.util.Enumeration nextElement  java.lang.Object [CALL] java.util.Enumeration hasMoreElements  boolean [CALL] java.util.Enumeration nextElement  java.lang.Object [CALL] java.util.Enumeration hasMoreElements  boolean [CALL] java.util.Enumeration nextElement  java.lang.Object [CALL] java.util.Enumeration hasMoreElements  boolean [CALL] java.util.Enumeration nextElement  java.lang.Object [CALL] java.util.Enumeration hasMoreElements  boolean [CALL] java.util.Enumeration nextElement  java.lang.Object [CALL] java.util.Enumeration hasMoreElements  boolean [CALL] java.util.Enumeration nextElement  java.lang.Object [CALL] java.util.Enumeration hasMoreElements  boolean [CALL] java.util.Enumeration nextElement  java.lang.Object [CALL] java.util.Enumeration hasMoreElements  boolean [CALL] java.util.Enumeration nextElement  java.lang.Object [CALL] java.util.Enumeration hasMoreElements  boolean [CALL] java.util.Enumeration nextElement  java.lang.Object [CALL] java.util.Enumeration hasMoreElements  boolean [CALL] java.util.Enumeration nextElement  java.lang.Object [CALL] java.util.Enumeration hasMoreElements  boolean [CALL] java.util.Enumeration nextElement  java.lang.Object [CALL] java.util.Enumeration hasMoreElements  boolean [CALL] java.util.Enumeration nextElement  java.lang.Object [CALL] java.util.Enumeration hasMoreElements  boolean [CALL] java.util.Enumeration nextElement  java.lang.Object [CALL] java.util.Enumeration hasMoreElements  boolean [CALL] java.util.Enumeration nextElement  java.lang.Object [CALL] java.util.Enumeration hasMoreElements  boolean [EXIT]").tokens

import numpy as np
from datasets import load_metric

metric = load_metric("accuracy")

def compute_metric(eval_pred):
    #print(eval_pred)
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    acc = metric.compute(predictions=eval_pred.predictions, references=eval_pred.label_ids)
    acc_2 = metric.compute(predictions=predictions, references=labels)
    print(acc)
    print(acc_2)
    return {'acc': acc, 'acc_2': acc_2}

from transformers import TrainingArguments
from transformers import DataCollatorForWholeWordMask
data_collator = DataCollatorForWholeWordMask(tokenizer=fast_tokenizer, mlm=True, mlm_probability=0.15)
training_args = TrainingArguments("test_trainer_bert_pre",
                                  num_train_epochs=1,
                                  prediction_loss_only=True,
                                  evaluation_strategy='steps'
)
from transformers import Trainer

trainer = Trainer(
    model=model,
    tokenizer=fast_tokenizer,
    data_collator=data_collator,
    args=training_args, train_dataset=encoded_dataset_train,
    eval_dataset=encoded_dataset_test,
    compute_metrics=compute_metric,
)

train_result = trainer.train(resume_from_checkpoint=True)
# train_result

trainer.save_model()
trainer_eval = trainer.evaluate(encoded_dataset_test, ignore_keys=['text'] ,metric_key_prefix='no')

print(trainer_eval)
metrics = train_result.metrics
print(metrics)
#trainer.log_metrics("train", compute_metrics)
#trainer.save_metrics("train", compute_metrics)
#trainer.save_state()

print(metric.inputs_description)


from transformers import pipeline

fill_mask = pipeline(
    "fill-mask",
    model="./test_trainer_bert_pre",
    tokenizer="./test_trainer_bert_pre"
)
result = fill_mask("setSuccessorInMultimap [ENTRY] void com.google.common.collect.LinkedHashMultimap$ValueEntry [MASK] [EXIT]")

print(result)